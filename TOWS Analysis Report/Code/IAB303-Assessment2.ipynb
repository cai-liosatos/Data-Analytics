{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAB303 - Assessment Task 2\n",
    "## TOWS analysis report\n",
    "\n",
    "#### INSTRUCTIONS\n",
    "\n",
    "1. Complete the section below with your personal details (and run the cell)\n",
    "2. Choose to use either the supplied scenario OR your own scenario. If selecting your own, check suitability with teaching team. If using the supplied scenario, use the provided internal data. You may supplement this with additional data as required.\n",
    "3. Ensure that you include at least 1 complete analysis using *internal* data\n",
    "4. Ensure that you include at least 1 complete analysis using *external* data\n",
    "5. Ensure that you include at least 1 actionable recommendation from a TOWS analysis using your data analytics from steps 3 & 4.\n",
    "6. Ensure that you use markdown cells to document your thinking and decision making for each stage of the process. Be clear on how your decisions are working towards addressing the business concern.\n",
    "7. Ensure that you undertakee a peer review process and complete the peer review section\n",
    "6. Before handing in your notebook, clear all cell outputs and run the complete notebook. Ensure that it runs without errors and that all output is displaying\n",
    "7. Right-click on your notebook name (in file viewer) and select download. Ensure that your name and student ID are on the file, and then upload to the appropriate assignment upload link in blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following cell with your details and run to produce your personalised header for this assignment\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "first_name = \"Cai\"\n",
    "last_name = \"Liosatos\"\n",
    "student_number = \"n10514295\"\n",
    "\n",
    "personal_header = \"<h1>\"+first_name+\" \"+last_name+\" (\"+student_number+\")</h1>\"\n",
    "display(HTML(personal_header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SCENARIO\n",
    "\n",
    "### Use the scenario below, OR write a description of your own scenario: \n",
    "\n",
    "You are working as a business analytics consultant for a non-profit organisation that offers residential aged care in Brisbane. The organisation has recently received philanthropic funding to help enance community understanding of the needs of the sector,  improve government action, and improve their influence within the sector.\n",
    "Before taking action and spending the money, they would like to have a better understanding of strengths and weaknesses in their service area through an analysis of relevant service providers. They have provided you with a report on [Service Places from 2021](https://www.gen-agedcaredata.gov.au/Resources/Access-data/2022/April/GEN-data-Providers,-services-and-places-in-aged-ca) for this purpose. They would also like to know about possible opportunity and threats which may impact the objective. They have suggested using data on [what Australian's think of aged care](https://data.gov.au/dataset/ds-dga-2eae3889-8a5e-413a-9496-5fd80f7ae370/details?q=aged%20care), supplemented by an analysis of relevant Australian headlines from [the Guardian](https://www.theguardian.com/au) online news.\n",
    "\n",
    "\n",
    "\n",
    "For the purposes of this exercise, you can make up other aspects of the scenario which may be important to your anlaysis (e.g. location, business details), and you may choose to use other sources of data if they are helpful.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### [1] Business Concern\n",
    "\n",
    "*## Expand on your interpretation of your chosen scenario here, and be clear in identifying the business concern that the analysis will address. ##*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enhance community understanding of the needs of the sector,  <br />\n",
    "improve government action, <br />\n",
    "improve their influence within the sector<br />\n",
    "\n",
    "strengths and weaknesses in their service area through an analysis of relevant service providers<br />\n",
    "opportunity and threats which may impact the objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "external:\n",
    "- covid\n",
    "- death\n",
    "- increasing prices\n",
    "- accessibility\n",
    "- staffing\n",
    "- legislation/laws\n",
    "- mandatory cultural/religious changes/assistance \n",
    "\n",
    "internal:\n",
    "- number of diff org types per state\n",
    "- remoteness\n",
    "- operational places\n",
    "- look at ACPR name, potentially look at total count/operational place for all of sydney, bris.....\n",
    "- accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed for this notebook here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "\n",
    "cache_on = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Services places:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ACPR code - code for 'place'\n",
    "- MMM code - 'remoteness' code for remoteness column, not 1:1\n",
    "- service size, need to iterate to make it readable, (just 1-20, 21-40, 41-60, 61-80, 81-100, 101+)\n",
    "- operational places - # of places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### survey:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight: population weight expressed as thousands of people in the Australian population aged 18 years\n",
    "or more (i.e. a value of 3 is 3,000 people).\n",
    "\n",
    "SCR1 Do you agree to participate in the survey?<br />\n",
    "SINGLE RESPONSE<br />\n",
    "1. Yes\n",
    "2. No\n",
    "\n",
    "SCR3 Do you currently live in<br />\n",
    "SINGLE RESPONSE<br />\n",
    "READ OUT<br />\n",
    "1. Residential aged care or a nursing home\n",
    "2. A place you own\n",
    "3. A place you rent\n",
    "4. Somewhere else\n",
    "\n",
    "HQGender: “allocated gender” for weighting from SCR4<br />\n",
    "1. 1 Male\n",
    "2. 2 Female\n",
    "\n",
    "Hqareatypex: ARIA derived from SCR5<br />\n",
    "1. Metro\n",
    "2. Regional\n",
    "3. Remote\n",
    "\n",
    "age bracket (scr7)<br />\n",
    "SCR7 To which of the following age groups do you belong?<br />\n",
    "SINGLE RESPONSE<br />\n",
    "READ OUT LIST<br />\n",
    "1. Under 18\n",
    "2. 18-24\n",
    "3. 25-34\n",
    "4. 35-44\n",
    "5. 45-54\n",
    "6. 55-64\n",
    "7. 65-69\n",
    "8. 70-79\n",
    "9. 80-89\n",
    "10. 90 or older\n",
    "99. Prefer not to say\n",
    "\n",
    "house income (scr9)<br />\n",
    "SCR9 I will read a list of income ranges, please tell me which one is the best estimate of your TOTAL<br />\n",
    "approximate annual income from all sources, before tax.<br />\n",
    "SINGLE RESPONSE<br />\n",
    "DO NOT READ PER WEEK UNLESS NEEDED<br />\n",
    "$1 to $9,999 per year ($1 - $189 per week) **1**<br />\n",
    "$10,000 - $19,999 per year ($190 - $379 per week) **2**<br />\n",
    "$20,000 - $29,999 per year ($380 - $579 per week) **3**<br />\n",
    "$30,000 - $39,999 per year ($580 - $769 per week) **4**<br />\n",
    "$40,000 - $49,999 per year ($770 - $959 per week) **5**<br />\n",
    "$50,000 - $59,999 per year ($960 - $1149 per week) **6**<br />\n",
    "$60,000 - $79,999 per year ($1150 - $1529 per week) **7**<br />\n",
    "$80,000 - $99,999 per year ($1530 - $1919 per week) **8**<br />\n",
    "$100,000 - $124,999 per year ($1920 - $2399 per week) **9**<br />\n",
    "$125,000 - $149,999 per year ($2400 - $2879 per week) **10**<br />\n",
    "$150,000 - $199,999 per year ($2880 - $3839 per week) **11**<br />\n",
    "$200,000 or more per year ($3840 or more per week) **12**<br />\n",
    "DON’T KNOW **98**<br />\n",
    "REFUSED **99**<br />\n",
    "\n",
    "q36/40: if 36/40 = 1, then number in 36/40_other, elif 99, then user does not know/refuses and nothing in other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] Analysis of External Data - Opportunities and Threats\n",
    "\n",
    "*## Include a full QDAVI cycle for your analysis. You must do at least one complete analysis on external data. Ensure that you document what you are doing and why you are doing it ##*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions to create required data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### functions for caching API search results for local use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a cached file containing the page pulled from the API\n",
    "def cache_save(folder_url, current_page, cache_list):\n",
    "    np.save(folder_url+str(current_page)+\".npy\", cache_list, allow_pickle=True, fix_imports=True)\n",
    "\n",
    "# function to load the cached files into a list\n",
    "def cache_load(folder_url):\n",
    "# loading cache\n",
    "    if os.path.exists(folder_url):\n",
    "        loaded_cache = []\n",
    "        for file in os.listdir(folder_url):\n",
    "            loaded_cache.append(np.load(folder_url+file, allow_pickle=True).tolist())\n",
    "        new_cache_list = flatten_list(loaded_cache)\n",
    "        return new_cache_list\n",
    "\n",
    "# function to flatten list of cached lists into a singular list\n",
    "def flatten_list(cache_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in cache_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### functions for calling to TheGuardian API, and cleaning the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the api URL to get the total page count\n",
    "def page_count(api_url_start, api_url_end):\n",
    "    error_counter = 0\n",
    "    while error_counter < 3:\n",
    "        api_url = api_url_start + \"1\" + api_url_end\n",
    "        content = requests.get(api_url)\n",
    "        api_data = json.loads(content.content)\n",
    "        if api_data['response']['status'] == \"ok\":\n",
    "            page_count = int(api_data['response']['pages'])\n",
    "            break\n",
    "        else:\n",
    "            error_counter += 1\n",
    "    page_count = \"API has errored three times in a row whilst trying to get page count, check if the URL is correct\" if error_counter >= 3 else page_count\n",
    "    return page_count\n",
    "\n",
    "#  function to scrape the data from the page returned from the api into a list\n",
    "def api_scraping(page_count, url_start, url_end, folder_url):\n",
    "    error_msg = []\n",
    "    error_counter = 0\n",
    "    current_page = 1\n",
    "    results_list = []\n",
    "# while loop to add outputs to a list\n",
    "    while current_page <= page_count:\n",
    "        if error_counter < 3:\n",
    "            cache_list = []\n",
    "            api_url = url_start + str(current_page) + url_end\n",
    "            content = requests.get(api_url)\n",
    "            api_data = json.loads(content.content)\n",
    "            if api_data['response']['status'] == \"error\":\n",
    "                error_msg.append(api_data['response']['message'])\n",
    "                error_counter += 1\n",
    "            else:\n",
    "                for item in api_data['response']['results']:\n",
    "                    cache_list.append(item)\n",
    "                    results_list.append(item)\n",
    "        \n",
    "                # caching results for local use\n",
    "                if cache_on:\n",
    "                    cache_save(folder_url, current_page, cache_list)\n",
    "                current_page += 1\n",
    "                error_counter = 0\n",
    "                error_msg = []\n",
    "        else:\n",
    "            error_msg.append(\"API has errored three times in a row, giving up\")\n",
    "            break\n",
    "    return results_list, error_msg\n",
    "\n",
    "# Clean up dataframe to be more visually appealing, and easier to use\n",
    "def df_creation(dataframe_name):\n",
    "    dataframe_name = dataframe_name.rename(columns = {'id':'ID', 'type':'Type', 'sectionId':'SectionID', 'sectionName':'Section Name', 'webPublicationDate':'Web Publication Date', 'webTitle':'Web Title', 'webUrl':'URL', 'apiUrl':'API URL', 'isHosted':'Is Hosted', 'pillarId': 'Pillar ID', 'pillarName':'Pillar Name'}).copy()\n",
    "    # clean the data to be more user friendly\n",
    "    if \"Web Publication Date\" in dataframe_name.columns:\n",
    "        dataframe_name[\"Web Publication Date\"] = dataframe_name[\"Web Publication Date\"].apply(lambda x: x.replace(\"T\", \" \").replace(\"Z\", \"\"))\n",
    "        if dataframe_name[\"Web Publication Date\"].dtype == object:\n",
    "            dataframe_name[\"Web Publication Date\"] = pd.to_datetime(dataframe_name[\"Web Publication Date\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        dataframe_name.sort_values(by='Web Publication Date', ascending=False, inplace=True)\n",
    "        dataframe_name[\"Month\"] = dataframe_name[\"Web Publication Date\"].dt.month\n",
    "        dataframe_name[\"Year\"] = dataframe_name[\"Web Publication Date\"].dt.year\n",
    "    if \"Pillar ID\" in dataframe_name.columns:\n",
    "        dataframe_name[\"Pillar ID\"] = dataframe_name[\"Pillar ID\"].astype(str).apply(lambda x: x.replace(\"pillar/\", \"\"))\n",
    "    \n",
    "    return dataframe_name\n",
    "\n",
    "def duplicate_check(original_df):\n",
    "    master_list = []\n",
    "    result_list = []\n",
    "    for index, row in original_df.iterrows():\n",
    "        dynamic_map = {}\n",
    "        dynamic_map[\"ID\"] = row[\"ID\"]\n",
    "        dynamic_map[\"Type\"] = row[\"Type\"]\n",
    "        dynamic_map[\"SectionID\"] = row[\"SectionID\"]\n",
    "        dynamic_map[\"Section Name\"] = row[\"Section Name\"]\n",
    "        dynamic_map[\"Web Publication Date\"] = row[\"Web Publication Date\"]\n",
    "        dynamic_map[\"Web Title\"] = row[\"Web Title\"]\n",
    "        dynamic_map[\"URL\"] = row[\"URL\"]\n",
    "        dynamic_map[\"API URL\"] = row[\"API URL\"]\n",
    "        dynamic_map[\"Is Hosted\"] = row[\"Is Hosted\"]\n",
    "        dynamic_map[\"Pillar ID\"] = row[\"Pillar ID\"]\n",
    "        dynamic_map[\"Pillar Name\"] = row[\"Pillar Name\"]\n",
    "        dynamic_map[\"Month\"] = row[\"Month\"]\n",
    "        dynamic_map[\"Year\"] = row[\"Year\"]\n",
    "        result_list.append(dynamic_map)\n",
    "\n",
    "    for item in range(len(result_list)):\n",
    "        if result_list[item] not in master_list:\n",
    "            master_list.append(result_list[item])  \n",
    "\n",
    "    new_df = pd.DataFrame(master_list)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### functions for HTML, and cleaning the constructed DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cleaning(df):\n",
    "    if df[\"Date\"].dtype == object:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df.sort_values(by='Date', ascending=False, inplace=True)\n",
    "    \n",
    "    if \"Month\" not in df.columns:\n",
    "        df[\"Month\"] = df[\"Date\"].dt.month\n",
    "    if \"Year\" not in df.columns:\n",
    "        df[\"Year\"] = df[\"Date\"].dt.year\n",
    "    return df\n",
    "\n",
    "# Get HTML function\n",
    "def get_HTML(url):\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    return html\n",
    "\n",
    "# Beautiful soup function for subtitle\n",
    "def extract_subTitle(HTML):\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\") # the html input and the parser name\n",
    "    article = soup.find(\"main\") # the tag that contains the article\n",
    "    div_element = article.find(\"div\", attrs={\"data-gu-name\": \"standfirst\"}) # the tag that can be found using an attribute\n",
    "    target_element = div_element.find(\"p\")\n",
    "    if target_element:\n",
    "        return target_element.text\n",
    "    else:\n",
    "        return '-'\n",
    "\n",
    "# Beautiful soup function for live articles\n",
    "def parse_article(article, temp_result):\n",
    "    fig_element = article.find(\"figure\")\n",
    "    if fig_element:\n",
    "        temp_result = ''\n",
    "    else:\n",
    "        for child in article.children:   \n",
    "            if child.name == 'p':\n",
    "                temp_result += child.text + '\\n'\n",
    "            if child.name == 'ul':\n",
    "                for li in child.findAll('li'):\n",
    "                    if li.find('ul'):\n",
    "                        break\n",
    "                    temp_result += li.text + '\\n'\n",
    "        temp_result += '\\n'\n",
    "    return temp_result\n",
    "\n",
    "# Beautiful soup function for body\n",
    "def extract_body(HTML):\n",
    "    result = \"\"\n",
    "    soup = BeautifulSoup(HTML, \"html.parser\") # the html input and the parser name\n",
    "\n",
    "    news = soup.find(\"main\", attrs={\"data-layout\": \"LiveLayout\"})\n",
    "    if news:\n",
    "        div_element = news.find(\"div\", attrs={\"id\": \"liveblog-body\"}) # the tag that can be found using an attribute\n",
    "        div_art_element = div_element.findAll(\"article\")\n",
    "        for item in div_art_element:\n",
    "            temp_result = ''\n",
    "            temp_result = parse_article(item, temp_result)\n",
    "            result += temp_result\n",
    "    else:\n",
    "        news = soup.find(\"main\") # the tag that contains the article\n",
    "        div_element = news.find(\"div\", attrs={\"id\": \"maincontent\"}) # the tag that can be found using an attribute\n",
    "        div_div_element = div_element.find(\"div\")\n",
    "        target_elements = div_element.findAll(\"p\")\n",
    "        for te in target_elements:\n",
    "            result += te.text + '\\n'*2\n",
    "            \n",
    "    return result\n",
    "\n",
    "def scraping_df(df):\n",
    "    results_data_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        dynamic_map = {}\n",
    "        dynamic_map[\"Date\"] = row[\"Web Publication Date\"]\n",
    "        dynamic_map[\"Section\"] = row[\"Section Name\"]\n",
    "        dynamic_map[\"Title\"] = row[\"Web Title\"]\n",
    "        dynamic_map[\"Subtitle\"] = extract_subTitle(get_HTML(row[\"URL\"]))\n",
    "        dynamic_map[\"Body\"] = extract_body(get_HTML(row[\"URL\"]))\n",
    "\n",
    "        results_data_list.append(dynamic_map)\n",
    "        \n",
    "    scraped_data_df = df_cleaning(pd.DataFrame(results_data_list))\n",
    "    \n",
    "    return scraped_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code using said functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pulling results from TheGuardian api from 2017 onwards under the search condition of \"Aged care facility\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting important variables\n",
    "aged_care_results = []\n",
    "aged_care_api_url_start = \"https://content.guardianapis.com/search?from-date=2017-01-01&order-by=newest&page=\"\n",
    "aged_care_api_url_end = \"&page-size=50&q=%22Aged%20care%20facility%22&api-key=dd3e21c9-be37-4bdb-b311-2fd86c0fd153\"\n",
    "\n",
    "# getting the total page count\n",
    "aged_care_page_count = page_count(aged_care_api_url_start, aged_care_api_url_end)\n",
    "# making the cache folder directory\n",
    "if cache_on:\n",
    "    if os.path.exists(\"./cache/Aged-Care-Facility/\") is False:\n",
    "        os.makedirs(\"./cache/Aged-Care-Facility/\")\n",
    "\n",
    "# printing first result of data if no errors, else printing the error (formatted)\n",
    "if type(aged_care_page_count) == int:\n",
    "    aged_care_results, error_msg = api_scraping(aged_care_page_count, aged_care_api_url_start, aged_care_api_url_end, \"./cache/Aged-Care-Facility/\")\n",
    "    print(f\"{error_msg[3]}\\n\\nThese were the error messages:\\n1: {error_msg[0]}\\n2: {error_msg[1]}\\n3: {error_msg[2]}\\n\\nMake sure the URL is correct, then try again\") if len(error_msg) > 3 else print(aged_care_results[0])\n",
    "else:\n",
    "    print(aged_care_page_count)\n",
    "\n",
    "\n",
    "\n",
    "# 'https://content.guardianapis.com/search?from-date=2017-01-01&order-by=newest&page=1&page-size=50&q=%22Aged%20care%20facility%22&api-key=dd3e21c9-be37-4bdb-b311-2fd86c0fd153'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading cache\n",
    "if cache_on:\n",
    "    aged_care_results = cache_load(\"./cache/Aged-Care-Facility/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_df = df_creation(pd.DataFrame(aged_care_results))\n",
    "ac_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pulling results from TheGuardian api from 2017 onwards under the search condition of \"residential aged care\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# setting important variables\n",
    "residential_care_results = []\n",
    "residential_care_api_url_start = \"https://content.guardianapis.com/search?from-date=2017-01-01&order-by=newest&page=\"\n",
    "residential_care_api_url_end = \"&page-size=50&q=%22Residential%20Aged%20Care%22&api-key=dd3e21c9-be37-4bdb-b311-2fd86c0fd153\"\n",
    "\n",
    "# getting the total page count\n",
    "residential_care_page_count = page_count(residential_care_api_url_start, residential_care_api_url_end)\n",
    "\n",
    "# making the cache folder directory\n",
    "if cache_on:\n",
    "    if os.path.exists(\"./cache/Aged-Care-Facility/\") is False:\n",
    "        os.makedirs(\"./cache/Residential-Aged-Care/\")\n",
    "\n",
    "# printing first result of data if no errors, else printing the error (formatted)\n",
    "if type(residential_care_page_count) == int:\n",
    "    residential_care_results, error_msg = api_scraping(residential_care_page_count, residential_care_api_url_start, residential_care_api_url_end, \"./cache/Residential-Aged-Care/\")\n",
    "    print(f\"{error_msg[3]}\\n\\nThese were the error messages:\\n1: {error_msg[0]}\\n2: {error_msg[1]}\\n3: {error_msg[2]}\\n\\nMake sure the URL is correct, then try again\") if len(error_msg) > 3 else print(residential_care_results[0])\n",
    "else:\n",
    "    print(residential_care_page_count)\n",
    "\n",
    "\n",
    "# 'https://content.guardianapis.com/search?from-date=2017-01-01&order-by=newest&page=1&page-size=50&q=%22Residential%20Aged%20Care%22&api-key=dd3e21c9-be37-4bdb-b311-2fd86c0fd153'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading cache\n",
    "if cache_on:\n",
    "    residential_care_results = cache_load(\"./cache/Residential-Aged-Care/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_df = df_creation(pd.DataFrame(residential_care_results))\n",
    "rc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two dataframes together, removing duplicate entires\n",
    "final_api_data = duplicate_check(pd.concat([ac_df, rc_df]))\n",
    "final_api_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using web scraping to retrieve certain aspects of the acquired theguardian articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scrape_df = scraping_df(final_api_data)\n",
    "final_scrape_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pulling the data from the provided survey csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "survey_df = pd.read_csv('for-release-community-attitudes-survey.csv', low_memory=False)\n",
    "survey_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions for searching through the DF's for keywords, and removing duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searches the inputted original list of data scraped from the API for the inputted keywords, then creates a new dataframe\n",
    "def keyword_search(new_df, keywords, check):\n",
    "    results_list = []\n",
    "    master_list = []\n",
    "\n",
    "    for index, row in new_df.iterrows():\n",
    "        # defining variables for later use\n",
    "        null_count = 0\n",
    "        keyword_map = {}\n",
    "        item_Date = row[\"Date\"]\n",
    "        item_Title = row[\"Title\"]\n",
    "        item_Subtitle = row[\"Subtitle\"]\n",
    "        item_Body = row[\"Body\"]\n",
    "        item_Month = row[\"Month\"]\n",
    "        item_Year = row[\"Year\"]\n",
    "\n",
    "        # searching for the keywords inputted\n",
    "        for keyword in keywords:\n",
    "            keyword_map[keyword] = item_Body.lower().count(keyword.lower())\n",
    "            if keyword_map[keyword] == 0:\n",
    "                null_count += 1\n",
    "        dynamic_object = {}\n",
    "\n",
    "        # setting column variables in dynamic_object dict based on found keywords\n",
    "        dynamic_object[\"Date\"] = item_Date\n",
    "        dynamic_object[\"Title\"] = item_Title\n",
    "        dynamic_object[\"Subtitle\"] = item_Subtitle\n",
    "        dynamic_object[\"Body\"] = item_Body\n",
    "        dynamic_object[\"Month\"] = item_Month\n",
    "        dynamic_object[\"Year\"] = item_Year\n",
    "\n",
    "        if check:\n",
    "            for keyword in keywords:\n",
    "                dynamic_object[keyword] = keyword_map[keyword]\n",
    "        # appending found articles to list\n",
    "        if null_count < len(keywords):\n",
    "            results_list.append(dynamic_object)\n",
    "\n",
    "    # removing duplicate entries from list (function can be called using multiple dataframes, and will work dynamically)\n",
    "    for item in range(len(results_list)):\n",
    "        if results_list[item] not in master_list:\n",
    "            master_list.append(results_list[item])\n",
    "            \n",
    "    # creating dataframe with found results\n",
    "    master_df = pd.DataFrame(master_list)\n",
    "    master_df.sort_values(by='Date', ascending=False, inplace=True)\n",
    "\n",
    "    return master_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions for creating histograms, and providing frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a simple histogram\n",
    "def create_hist(df, col, title, xlabel, ylabel):\n",
    "    bins = list(range(df[col].min(), df[col].max()+2))\n",
    "    hist = df[col].hist(bins=bins)\n",
    "    hist.set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    return hist\n",
    "\n",
    "# Function to create df containing the frequency of unique values in a given column in a given dataframe\n",
    "def count_generator(df, col):\n",
    "    list = []\n",
    "    number = int(df[col].min())\n",
    "    iterate_count = int(df[col].max()) - int(df[col].min()) + 1\n",
    "\n",
    "    # iterate in the amount of the number of unique values\n",
    "    for item in range(iterate_count):\n",
    "        map = {}\n",
    "        item_Month = number\n",
    "\n",
    "        if number in df[col].values:\n",
    "            item_Count = df[col].value_counts()[number]\n",
    "        else:\n",
    "            item_Count = 0\n",
    "\n",
    "        # append values to object, then to list\n",
    "        map[col] = item_Month\n",
    "        map[\"Count\"] = item_Count\n",
    "        list.append(map)\n",
    "        number +=1\n",
    "    new_df = pd.DataFrame(list)\n",
    "    return new_df\n",
    "\n",
    "def multi_column_counter(df):\n",
    "    list = []\n",
    "    number = 1\n",
    "    \n",
    "    for item in range(12):\n",
    "        map = {}\n",
    "        item_Month = number\n",
    "        \n",
    "        if number in df[\"Month\"].values:\n",
    "            df2 = df[(df[\"Month\"] == number)]\n",
    "            item_death = df2['death'].sum()\n",
    "            item_covid = df2['covid'].sum()\n",
    "            item_corona = df2['corona'].sum()\n",
    "            item_increase = df2['increase'].sum()\n",
    "            item_staff = df2['staff'].sum()\n",
    "            item_laws = df2['laws'].sum()\n",
    "            item_legislat = df2['legislat'].sum()\n",
    "        else:\n",
    "            item_death = 0\n",
    "            item_covid = 0\n",
    "            item_corona = 0\n",
    "            item_increase = 0\n",
    "            item_staff = 0\n",
    "            item_laws = 0\n",
    "            item_legislat = 0\n",
    "\n",
    "        # append values to object, then to list\n",
    "        map[\"Month\"] = number\n",
    "        map[\"death\"] = item_death\n",
    "        map[\"covid\"] = item_covid\n",
    "        map[\"corona\"] = item_corona\n",
    "        map[\"increase\"] = item_increase\n",
    "        map[\"staff\"] = item_staff\n",
    "        map[\"laws\"] = item_laws\n",
    "        map[\"legislat\"] = item_legislat\n",
    "        list.append(map)\n",
    "        number +=1\n",
    "    return pd.DataFrame(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survey_analysis(df, questions, questions_answers, gender):\n",
    "    df_list = []\n",
    "    age_map = {1: \"Under 18\", 2: \"18-24\", 3: \"25-34\", 4: \"35-44\", 5: \"45-54\", 6: \"55-64\", 7: \"65-69\", 8: \"70-79\", 9: \"80-89\", 10: \"90 or older\", 99: \"Prefer not to say\"}\n",
    "    for question in questions:\n",
    "        index = questions.index(question)        \n",
    "        result_list = []\n",
    "        for answer in list(range(questions_answers[index][0], questions_answers[index][1] + 1)):\n",
    "            dynamic_map = {} \n",
    "            dynamic_map[\"Answers\"] = answer\n",
    "            for k, v in age_map.items():\n",
    "                dynamic_map[v] = len(df.loc[(df[question] == answer) & (df[\"Age Bracket\"] == k) & (df[\"HQGender\"] == gender)])\n",
    "            result_list.append(dynamic_map)\n",
    "        df_list.append(pd.DataFrame(result_list))\n",
    "        \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def survey_analysis(df, questions, questions_answers):\n",
    "#     df_list = []\n",
    "#     age_map = {1: \"Under 18\", 2: \"18-24\", 3: \"25-34\", 4: \"35-44\", 5: \"45-54\", 6: \"55-64\", 7: \"65-69\", 8: \"70-79\", 9: \"80-89\", 10: \"90 or older\", 99: \"Prefer not to say\"}\n",
    "#     for question in questions:\n",
    "#         index = questions.index(question)        \n",
    "#         for gender in range(2):\n",
    "#             result_list = [[]]\n",
    "#             for answer in list(range(questions_answers[index][0], questions_answers[index][1] + 1)):\n",
    "#                 dynamic_map = {} \n",
    "#                 dynamic_map[\"Answers\"] = answer\n",
    "#                 for k, v in age_map.items():\n",
    "#                     dynamic_map[v] = len(df.loc[(df[question] == answer) & (df[\"Age Bracket\"] == k) & (df[\"HQGender\"] == gender + 1)])\n",
    "#                 result_list[gender].append(dynamic_map)\n",
    "#             df_list.append(pd.DataFrame(result_list))\n",
    "        \n",
    "#     return df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coding using said functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### checking scraped results for relevant keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify desired keywords (artciles relating the war in ukraine, and GEI/fuel issues)\n",
    "\n",
    "# create DF of keywords (and count) in GRI issues data\n",
    "keywords_df1 = keyword_search(final_scrape_df, [\"death\", \"covid\", \"corona\", \"increase\", \"staff\", \"laws\", \"legislat\"], True)\n",
    "\n",
    "keywords_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify desired keywords (artciles relating the war in ukraine, and GEI/fuel issues)\n",
    "\n",
    "# create DF of keywords (and count) in GRI issues data\n",
    "keywords_df2 = keyword_search(final_scrape_df, [\"death\", \"covid\", \"corona\", \"increase\", \"staff\", \"laws\", \"legisla\"], False)\n",
    "\n",
    "keywords_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### creating new DFs only containing relevant information for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_keywords_list = []\n",
    "year_keywords_count_list = []\n",
    "for i in range(int(final_scrape_df[\"Year\"].min()), int(final_scrape_df[\"Year\"].max())+1):\n",
    "    keywords_df = keywords_df1.loc[keywords_df1['Year'] == i].sort_values(by=\"Month\", ascending=True)\n",
    "    year_keywords_list.append(keywords_df)\n",
    "    year_keywords_count_list.append(multi_column_counter(keywords_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df_list = []\n",
    "year_count_list = []\n",
    "for i in range(int(final_scrape_df[\"Year\"].min()), int(final_scrape_df[\"Year\"].max())+1):\n",
    "    year_df = keywords_df1.loc[keywords_df1['Year'] == i].sort_values(by=\"Month\", ascending=True)\n",
    "    year_df_list.append(keywords_df)\n",
    "    year_count_list.append(count_generator(year_df, \"Month\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of data from survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = [\"Q2_I\", \"Q3\", \"Q18_C\", \"Q18_D\", \"Q20_B\", \"Q26_C\", \"Q27\", \"Q32A_A\", \"Q32A_B\", \"Q32A_C\", \"Q32A_D\", \"Q32A_E\", \"Q32A_F\", \"Q32A_G\", \"Q32A_H\", \"Q32A_I\", \"Q32A_J\", \"Q32b_A\", \"Q32b_B\", \"Q32b_C\", \"Q32b_D\", \"Q32b_E\", \"Q32b_F\", \"Q32b_G\", \"Q32b_H\", \"Q32b_I\", \"Q32b_J\", \"Q36\"]\n",
    "questions_answers_list = [[1, 3], [1, 5], [1, 3], [1, 3], [1, 3], [1, 3], [1, 13], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 3], [1, 99]] \n",
    "survey_male_analysis_df_list = survey_analysis(survey_df, questions_list, questions_answers_list, 1)\n",
    "survey_female_analysis_df_list = survey_analysis(survey_df, questions_list, questions_answers_list, 2)\n",
    "\n",
    "survey_analysis_df_list = [survey_male_analysis_df_list, survey_female_analysis_df_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for m/f\n",
    "s1\n",
    "- scr3\n",
    "- age )scrsomething)\n",
    "- Q2_i\n",
    "- Q3\n",
    "--\n",
    "s4?\n",
    "Q18_C\n",
    "Q18_D\n",
    "Q20_B\n",
    "--\n",
    "s6\n",
    "Q27\n",
    "Q26_C\n",
    "\n",
    "Q32_A\n",
    "Q32_B\n",
    "\n",
    "Q36\n",
    "\n",
    "\n",
    "q5\n",
    "q14\n",
    "q24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the politicians\n",
    "fig, (ax0, ax1, ax2, ax3, ax4, ax5) = plt.subplots(nrows=6,  ncols=1)\n",
    "fig.suptitle(\"Frequency of Article Publications relating\\nto aged care services\", fontweight=\"bold\", size=15)\n",
    "fig_list = [ax0, ax1, ax2, ax3, ax4, ax5]\n",
    "year = final_scrape_df[\"Year\"].min()\n",
    "\n",
    "for count, df in enumerate(year_count_list):\n",
    "    fig_list[count].set_title(f\"Frequency of Article Publication by Month in {year}\", fontweight=\"bold\", size=13) \n",
    "    df.plot('Month', 'Count', ax=fig_list[count], label=\"Count\", xticks=list(range(df[\"Month\"].min(), df[\"Month\"].max()+1)), xlabel=\"Month\", ylabel=\"Frequency\", figsize=(5,10))\n",
    "    fig_list[count].legend(loc='upper right')\n",
    "    year += 1\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the politicians\n",
    "fig, (ax0, ax1, ax2, ax3, ax4, ax5) = plt.subplots(nrows=6,  ncols=1, figsize=(10,15))\n",
    "fig.suptitle(\"Frequency of Article Publications per year with certain keywords\", fontweight=\"bold\", size=15)\n",
    "fig_list = [ax0, ax1, ax2, ax3, ax4, ax5]\n",
    "year = final_scrape_df[\"Year\"].min()\n",
    "\n",
    "for count, df in enumerate(year_keywords_count_list):\n",
    "    fig_list[count].set_title(f\"Frequency of Article Publication by Month in {year}\", fontweight=\"bold\", size=13) \n",
    "    df.plot(\"Month\", [\"death\", \"covid\", \"corona\", \"increase\", \"staff\", \"laws\", \"legislat\"], ax=fig_list[count], kind=\"bar\", xlabel=\"Month\", ylabel=\"Frequency\")\n",
    "    fig_list[count].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    year += 1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = [\"Q2_I\", \"Q3\", \"Q18_C\", \"Q18_D\", \"Q20_B\", \"Q26_C\", \"Q27\", \"Q32A_A\", \"Q32A_B\", \"Q32A_C\", \"Q32A_D\", \"Q32A_E\", \"Q32A_F\", \"Q32A_G\", \"Q32A_H\", \"Q32A_I\", \"Q32A_J\", \"Q32b_A\", \"Q32b_B\", \"Q32b_C\", \"Q32b_D\", \"Q32b_E\", \"Q32b_F\", \"Q32b_G\", \"Q32b_H\", \"Q32b_I\", \"Q32b_J\", \"Q36\"]\n",
    "\n",
    "# Visualise the politicians\n",
    "fig, ((ax0, ax1), (ax2, ax3), (ax4, ax5), (ax6, ax7), (ax8, ax9), (ax10, ax11), (ax12, ax13), (ax14, ax15), (ax16, ax17), (ax18, ax19), \n",
    "    (ax20, ax21), (ax22, ax23), (ax24, ax25), (ax26, ax27), (ax28, ax29), (ax30, ax31), (ax32, ax33), (ax34, ax35), (ax36, ax37), (ax38, ax39), \n",
    "    (ax40, ax41), (ax42, ax43), (ax44, ax45), (ax46, ax47), (ax48, ax49)) = plt.subplots(nrows=25, ncols=2, constrained_layout = True)\n",
    "fig.suptitle(\"Survey Responses by Gender\", fontweight=\"bold\", size=20, y=18.15)\n",
    "fig.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=18, wspace=None, hspace=None)\n",
    "\n",
    "fig_list = [ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14, ax15, ax16, ax17, ax18, ax19, ax20, ax21, \n",
    "    ax22, ax23, ax24, ax25, ax26, ax27, ax28, ax29, ax30, ax31, ax32, ax33, ax34, ax35, ax36, ax37, ax38, ax39, ax40, ax41, ax42, ax43, ax44, \n",
    "    ax45, ax46, ax47, ax48, ax49]\n",
    "\n",
    "fig_count = 0\n",
    "if len(survey_analysis_df_list) > 0:\n",
    "    for index in range(len(survey_analysis_df_list[0])):\n",
    "        if len(survey_analysis_df_list[0][index]) != 3:\n",
    "            continue\n",
    "        \n",
    "        fig_list[fig_count].set_title(f\"Male responses to survey {questions_list[index]}\", fontweight=\"bold\", size=12)\n",
    "        survey_analysis_df_list[0][index].plot(\"Answers\", [\"Under 18\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65-69\", \"70-79\", \"80-89\", \"90 or older\", \"Prefer not to say\"], ax=fig_list[fig_count], kind=\"bar\", xlabel=\"Answer\", ylabel=\"Frequency\", figsize=(15,5))\n",
    "        fig_list[fig_count].legend().remove()\n",
    "        fig_count += 1\n",
    "\n",
    "        fig_list[fig_count].set_title(f\"Female responses to survey {questions_list[index]}\", fontweight=\"bold\", size=12)\n",
    "        survey_analysis_df_list[1][index].plot(\"Answers\", [\"Under 18\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65-69\", \"70-79\", \"80-89\", \"90 or older\", \"Prefer not to say\"], ax=fig_list[fig_count], kind=\"bar\", xlabel=\"Answer\", ylabel=\"Frequency\", figsize=(15,5))\n",
    "        fig_list[fig_count].legend(ncol=1, labelspacing=0., bbox_to_anchor=(1.4, .7), borderaxespad=0.)\n",
    "        fig_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] Analysis of Internal Data - Strengths and Weaknesses\n",
    "\n",
    "*## Include a full QDAVI cycle for your analysis. You must do at least one complete analysis on internal data. Ensure that you document what you are doing and why you are doing it  ##*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "GEN_2021_data_df = pd.read_csv('ServicesPlaces_2020to2021_GENdata.csv')\n",
    "\n",
    "GEN_2021_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_2020_data_df = pd.read_csv('ServicesPlaces_2019to2020_GENdata.csv', encoding = 'cp1252')\n",
    "GEN_2020_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_2019_data_df = pd.read_csv('Services-and-places-in-aged-care-30-June-2019.csv', encoding = 'cp1252')\n",
    "GEN_2019_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4] TOWS analysis - actionable recommendations\n",
    "\n",
    "*## Using your analytics from [1] and [2], perform a TOWS analysis to identify actionable recommendations. You must complete at least one quadrant of TOWS. Elaborate on your recommendation/s linking to the analysis, and also ensuring a meaningful connection to the business concern. ##*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### [5] Peer review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback received - reviewer 1: Firstname Lastname (Student number)\n",
    "\n",
    "*## Write comments here ##*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback received - reviewer 2: Firstname Lastname (Student number)\n",
    "\n",
    "*## Write comments here ##*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback received - reviewer 3: Firstname Lastname (Student number)\n",
    "\n",
    "*## Write comments here ##*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response to feedback received:\n",
    "\n",
    "*## Write response here ##*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback given to reviewer 1:\n",
    "\n",
    "*## Write comments here ##*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback given to reviewer 2:\n",
    "\n",
    "*## Write comments here ##*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback given to reviewer 3:\n",
    "\n",
    "*## Write comments here ##*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9808b2779335f0c2807c1ba1d02e59bcffca5136fddd2b56b75a42a062051586"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
